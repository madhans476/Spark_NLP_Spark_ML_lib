{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO1AUnHOvk0r",
        "outputId": "3e99c98b-bbaa-4996-c00d-81418e55061e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-15 18:38:21--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 3.86.22.73\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|3.86.22.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-11-15 18:38:21--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-15 18:38:21 (42.6 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.5.1\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.5.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.6/626.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwSBgQJ_vtYO",
        "outputId": "3b03fc12-6b13-4a8c-b8d9-0e104c2609c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.2.3)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
        "from sparknlp.annotator import BertEmbeddings, Tokenizer\n",
        "import pandas as pd\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n"
      ],
      "metadata": {
        "id": "K-l2Wyy9v7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init() # Initialize Spark\n",
        "# from pyspark.sql import SparkSession\n",
        "# Start Spark session with Spark NLP\n",
        "spark = sparknlp.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDchQZqMwDqy",
        "outputId": "6c3f2d9c-d888-4031-9ba0-bf489edfc963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df1 = pd.read_csv(\"hdfs://MADHANS:8020/user/cluster/incident_data.csv\")\n",
        "\n",
        "# Define schema for PySpark DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"hazard-category\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Convert pandas DataFrame to PySpark DataFrame\n",
        "df = spark.createDataFrame(df1, schema)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xTfqRO3PwDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDN2clapwJbU",
        "outputId": "2ce011c3-61fe-47ba-f5d5-d302a0b26e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 5.5.1\n",
            "Apache Spark version: 3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh97CmHawMJ2",
        "outputId": "ec335c16-263f-4fb0-99b4-a3ba6ad30cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+\n",
            "|                text|hazard-category|\n",
            "+--------------------+---------------+\n",
            "|Case Number: 011-...|      allergens|\n",
            "+--------------------+---------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Create a Document Assembler (for text preprocessing)\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Tokenizer to split the text into tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Step 3: Define the BERT Embeddings Model\n",
        "bert_embeddings = BertEmbeddings.pretrained(\"bert_base_uncased\", \"en\") \\\n",
        "    .setInputCols([\"document\", \"token\"]) \\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# Step 4: Use EmbeddingsFinisher to aggregate the embeddings as a vector\n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "    .setInputCols(\"embeddings\") \\\n",
        "    .setOutputCols(\"embeds\") \\\n",
        "    .setOutputAsVector(True) \\\n",
        "    .setCleanAnnotations(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyhMQPHZwQgX",
        "outputId": "8e98c618-42de-47dd-b577-c93e33140e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_uncased download started this may take some time.\n",
            "Approximate size to download 392.5 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Average the embeddings (using the embeddings from all tokens in a document)\n",
        "\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, FloatType\n",
        "import numpy as np\n",
        "\n",
        "# Define UDF to average the embeddings for each document\n",
        "def average_embeddings(vectors):\n",
        "    return np.mean(vectors, axis=0).tolist()  # Averaging across token embeddings\n",
        "\n",
        "average_embeddings_udf = udf(average_embeddings, ArrayType(FloatType()))\n",
        "\n",
        "# Step 6: Define the pipeline with the stages\n",
        "pipeline = Pipeline(stages=[document_assembler, tokenizer, bert_embeddings, embeddings_finisher])\n",
        "\n",
        "# Step 7: Apply the pipeline to your DataFrame\n",
        "result_df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Step 8: Average the embeddings for each document (optional: use CLS token for document representation)\n",
        "result_df = result_df.withColumn(\"vectors\", average_embeddings_udf(col(\"embeds\")))\n",
        "result_df = result_df.drop(\"embeds\")\n"
      ],
      "metadata": {
        "id": "TM3GTLBCwTqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6b_JUarwWZ7",
        "outputId": "9e0912d3-0c4c-4ca6-b86b-7b064179214b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+--------------------+\n",
            "|                text|hazard-category|             vectors|\n",
            "+--------------------+---------------+--------------------+\n",
            "|Case Number: 011-...|      allergens|[-0.41468012, -0....|\n",
            "+--------------------+---------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler"
      ],
      "metadata": {
        "id": "PAR9_VMixT_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Index the label column (hazard-category)\n",
        "indexer = StringIndexer(inputCol=\"hazard-category\", outputCol=\"label\")\n",
        "indexed_df = indexer.fit(result_df).transform(result_df)\n",
        "indexer_model = indexer.fit(result_df)"
      ],
      "metadata": {
        "id": "-jks_VNWySSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics # Import for confusion matrix\n",
        "# Define a UDF to convert array<float> to DenseVector\n",
        "def array_to_vector(array):\n",
        "    return Vectors.dense(array)\n",
        "\n",
        "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
        "\n",
        "# Apply the UDF to create a new column \"features\" as DenseVector\n",
        "assembled_df = indexed_df.withColumn(\"features\", array_to_vector_udf(col(\"vectors\")))\n",
        "\n",
        "# Scale features to the range [0, 1] using MinMaxScaler\n",
        "# scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "# scalerModel = scaler.fit(assembled_df)\n",
        "# scaled_df = scalerModel.transform(assembled_df)\n",
        "scaled_df = assembled_df\n",
        "\n",
        "# Step 3: Split the data into training and test sets\n",
        "train_df, test_df = scaled_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Define an evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n"
      ],
      "metadata": {
        "id": "1Gg0oo7-yVE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "lr_model = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
        "\n",
        "# Create a pipeline\n",
        "lr_pipeline = Pipeline(stages=[lr_model])\n",
        "\n",
        "# Train the model\n",
        "lr_trained_model = lr_pipeline.fit(train_df)\n",
        "\n",
        "# Predict on test data\n",
        "lr_predictions = lr_trained_model.transform(test_df)\n",
        "\n",
        "# Evaluate accuracy\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "lr_predictionAndLabels = lr_predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (x.prediction, x.label))\n",
        "lr_metrics = MulticlassMetrics(lr_predictionAndLabels)\n",
        "print(\"Logistic Regression Confusion Matrix:\")\n",
        "print(lr_metrics.confusionMatrix().toArray())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBdir6NUyh_t",
        "outputId": "db56f829-45e3-4154-de97-d7c50b4cade1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6642\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "warnings.warn(\n",
            "Logistic Regression Confusion Matrix:\n",
            "[[  3. 152.   4.   8.   3.   1.   0.   0.   0.   0.]\n",
            " [189.   4.   1.   2.   0.   0.   0.   0.   0.   0.]\n",
            " [  3.   6.  62.  10.   0.   0.   0.   0.   0.   0.]\n",
            " [ 18.  21.  30.   0.  10.   0.   0.   0.   0.   0.]\n",
            " [  0.   5.   1.  65.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   8.   2.  27.   0.   0.   0.   0.   0.]\n",
            " [  5.  13.  12.   0.  10.   0.   0.   0.   0.   0.]\n",
            " [  6.   5.   2.   0.   2.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   7.   0.   1.   8.   0.   0.   0.   0.]\n",
            " [  2.   0.   1.   0.   0.   2.   8.   0.   0.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Define the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create a pipeline\n",
        "dt_pipeline = Pipeline(stages=[dt_model])\n",
        "\n",
        "# Train the model\n",
        "dt_trained_model = dt_pipeline.fit(train_df)\n",
        "\n",
        "# Predict on test data\n",
        "dt_predictions = dt_trained_model.transform(test_df)\n",
        "\n",
        "# Evaluate accuracy\n",
        "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "dt_predictionAndLabels = dt_predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (x.prediction, x.label))\n",
        "dt_metrics = MulticlassMetrics(dt_predictionAndLabels)\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(dt_metrics.confusionMatrix().toArray())\n"
      ],
      "metadata": {
        "id": "M1FX6PqjyxLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30467ab0-2af0-450e-dfca-087e44174139"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy:0.5113\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "warnings.warn(\n",
            "Decision Tree Confusion Matrix\n",
            "[[132.  19.  52.   1.   0.   1.   0.   0.   0.   0.]\n",
            " [ 21.  95.  63.   1.   0.   2.   0.   0.   0.   0.]\n",
            " [ 14.   7.  66.   4.   0.   0.   0.   0.   0.   0.]\n",
            " [ 14.   7.  43.  15.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.  71.   0.   0.   0.   0.   0.]\n",
            " [ 10.   8.  27.   0.   0.   4.   0.   0.   0.   0.]\n",
            " [ 13.   4.  24.   4.   0.   0.   0.   0.   0.   0.]\n",
            " [  2.   2.  10.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   8.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  2.   0.   3.   0.   0.   0.   8.   0.   0.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create a pipeline\n",
        "rf_pipeline = Pipeline(stages=[rf_model])\n",
        "\n",
        "# Train the model\n",
        "rf_trained_model = rf_pipeline.fit(train_df)\n",
        "\n",
        "# Predict on test data\n",
        "rf_predictions = rf_trained_model.transform(test_df)\n",
        "\n",
        "# Evaluate accuracy\n",
        "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "rf_predictionAndLabels = rf_predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (x.prediction, x.label))\n",
        "rf_metrics = MulticlassMetrics(rf_predictionAndLabels)\n",
        "print(\"Random Forest Confusion Matrix:\")\n",
        "print(rf_metrics.confusionMatrix().toArray())\n"
      ],
      "metadata": {
        "id": "uA7PYaSGy03Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d4edf5-1eb4-4e79-ab3e-fb2426fdca46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.5634\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "warnings.warn(\n",
            "Random Forest Confusion Matrix:\n",
            "[[176.  14.   4.   2.   0.   0.   0.   0.   0.   0.]\n",
            " [ 23. 118.  14.   8.   5.   3.   0.   0.   0.   0.]\n",
            " [  3.   6.  62.  10.   0.   0.   0.   0.   0.   0.]\n",
            " [ 18.  21.  30.   0.  10.   0.   0.   0.   0.   0.]\n",
            " [  9.   5.   4.  52.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   8.   2.  27.   0.   0.   0.   0.   0.]\n",
            " [  13.  2.  15.   0.  10.   0.   0.   0.   0.   0.]\n",
            " [  6.   5.   2.   0.   2.   0.   0.   0.   0.   0.]\n",
            " [  5.   3.   7.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  2.   3.   1.   4.   0.   2.   1.   0.   0.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "\n",
        "# Define the Neural Network model\n",
        "nn_model = MultilayerPerceptronClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    layers=[768, 512, 256, 128, 64, len(indexer_model.labels)]\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "nn_pipeline = Pipeline(stages=[nn_model])\n",
        "\n",
        "# Train the model\n",
        "nn_trained_model = nn_pipeline.fit(train_df)\n",
        "\n",
        "# Predict on test data\n",
        "nn_predictions = nn_trained_model.transform(test_df)\n",
        "\n",
        "# Evaluate accuracy\n",
        "nn_accuracy = evaluator.evaluate(nn_predictions)\n",
        "print(f\"Neural Network Accuracy: {nn_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "nn_predictionAndLabels = nn_predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (x.prediction, x.label))\n",
        "nn_metrics = MulticlassMetrics(nn_predictionAndLabels)\n",
        "print(\"Neural Network Confusion Matrix:\")\n",
        "print(nn_metrics.confusionMatrix().toArray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVR9uNQ4y37_",
        "outputId": "377e882a-fc0b-496c-ccc7-1868a2545507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Accuracy: 0.7250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Confusion Matrix:\n",
            "[[192.   4.   0.   7.   0.   2.   0.   0.   0.   0.]\n",
            " [  2. 159.   3.   7.   0.  11.   0.   0.   0.   0.]\n",
            " [  3.   6.  62.  10.   0.  10.   0.   0.   0.   0.]\n",
            " [ 16.   6.  21.  32.   0.   4.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.  71.   0.   0.   0.   0.   0.]\n",
            " [  0.  12.   8.   2.   0.  27.   0.   0.   0.   0.]\n",
            " [  5.   5.  13.  12.   0.  10.   0.   0.   0.   0.]\n",
            " [  2.   5.   3.   1.   0.   3.   0.   0.   0.   0.]\n",
            " [  0.   0.   7.   0.   0.   1.   0.   0.   0.   0.]\n",
            " [  1.   0.   2.   0.   0.   2.   0.   0.   0.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "pmo6xrpA_jbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}